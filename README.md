**Relation DETR**
===

By [Xiuquan Hou](https://github.com/xiuqhou), [Meiqin Liu](https://scholar.google.com/citations?user=T07OWMkAAAAJ&hl=zh-CN&oi=ao), Senlin Zhang, [Ping Wei](https://scholar.google.com/citations?user=1OQBtdcAAAAJ&hl=zh-CN&oi=ao), [Badong Chen](https://scholar.google.com/citations?user=mq6tPX4AAAAJ&hl=zh-CN&oi=ao), Xuguang Lan.

This repo is the official implementation of [Relation DETR: Exploring Explicit Position Relation Prior for Object Detection](https://arxiv.org/abs/2407.11699v1) accepted to **ECCV2024** (score **5444**, **oral** presentation). [[Arxiv paper link](https://arxiv.org/abs/2407.11699)], [[ËÆ∫Êñá‰ªãÁªç](https://www.bilibili.com/video/BV13m421377m)], [[‰ª£Á†ÅËÆ≤Ëß£](https://www.bilibili.com/video/BV1uf421B7e7)]

üíñ If our [Relation-DETR](https://github.com/xiuqhou/Relation-DETR) or [SA-Det-100k](https://huggingface.co/datasets/xiuqhou/SA-Det-100k/tree/main) dataset is helpful to your researches or projects, please star this repository. Thanks! ü§ó

<div align="center">
    <img src="images/convergence_curve.png" width=500>
</div>

## TODO

...Want more features? Open a [Feature Request](https://github.com/xiuqhou/Relation-DETR/issues/new?assignees=&labels=enhancement&projects=&template=feature-request-english.yml).

- [x] Support data augmentations from `albumentations`.
- [ ] Support Mosaic and Mixup data augmentation.
- [ ] More detailed docs for the code.
- [ ] Add a instruction about introducing our relation to other models.
- [ ] Support GradCam and feature visualization.
- [x] Upload more pretrained weights and training logs.
- [x] Update visualization code for MC.
- [x] Update Model ZOO.

## Update

- [2024-09-08] Relation-DETR (Focal-Large) checkpoint pretrained on [Object365](https://www.objects365.org/overview.html) is now available [here](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_focalnet_large_lrf_fl4_800_1333_o365_4e.pth).
- [2024-08-15] We release [config](configs/dino++/dino++_resnet50_800_1333.py) and [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.1/dino++_resnet50_800_1333_coco_1x.pth) of DINO++ ([DINO](https://github.com/IDEA-Research/DINO) enhanced by our position relation).
- [2024-08-12] Relation-DETR is selected for **Oral** presentation in ECCV2024!
- [2024-08-11] The pretrained weight for Relation-DETR on SA-Det-100k are available [here](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_resnet50_800_1333_sa_det_100k_1x.pth)!
- [2024-08-07] Relation-DETR with [FocalNet-large](https://github.com/microsoft/FocalNet/) achieves `63.5AP` on COCO test-dev2017 after fine-tuned for 4 epochs on [Object365](https://www.objects365.org/overview.html), [config](configs/relation_detr/relation_detr_focalnet_large_lrf_fl4_1200_2000.py) and [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_focalnet_large_lrf_fl4_1200_2000_o365_4e-coco_2x.pth) are available now!
- [2024-07-24] Upload SA-Det-100k dataset, see it in [Hugging Face](https://huggingface.co/datasets/xiuqhou/SA-Det-100k/tree/main) and [Ai Studio](https://aistudio.baidu.com/datasetdetail/286554).
- [2024-07-18] Upload Relation-DETR [training logs](https://github.com/xiuqhou/Relation-DETR/issues/2) for pretrained weights.
- [2024-07-18] We release the code for [Relation-DETR](https://arxiv.org/abs/2407.11699v1), Relation-DETR with Swin-L achieves **58.1 AP**!
- [2024-03-26] Code for [Salience-DETR](https://arxiv.org/abs/2403.16131) is available [here](https://github.com/xiuqhou/Salience-DETR).
- [2024-07-17] We release the checkpoint for Relation-DETR with ResNet-50 and Swin-L backbones, see [Releases v1.0.0](https://github.com/xiuqhou/Relation-DETR/releases/tag/v1.0.0).
- [2024-07-01] Relation-DETR is accepted to ECCV2024. Welcome to your attention!

## SA-Det-100k

[SA-Det-100k](https://github.com/xiuqhou/Relation-DETR) is a large-scale class-agnostic object detection dataset for **Research Purposes only**. The dataset is based on a subset of [SA-1B](https://ai.meta.com/datasets/segment-anything/) (see [LICENSE](https://ai.meta.com/datasets/segment-anything-downloads/)), and all objects belong to the same category `objects`. Because it contains a large number of scenarios but does not provide class-specific annotations, we believe it may be a good choice to pre-training models for a variety of downstream tasks with different categories. The dataset contains about 100k images, and each image is resized using [opencv-python](https://pypi.org/project/opencv-python/) so that the larger one of their height and width is 1333, which is consistent with the data augmentation commonly used to train [COCO](https://cocodataset.org/). The dataset can be found in:

- [Hugging Face](https://huggingface.co/datasets/xiuqhou/SA-Det-100k/tree/main)
- [Ai Studio](https://aistudio.baidu.com/datasetdetail/286554)

## Model ZOO

### COCO


| Model               | Backbone             | Epoch |                                                                                                                                                 Download                                                                                                                                                 |  mAP  | AP<sub>50 | AP<sub>75 | AP<sub>S | AP<sub>M | AP<sub>L |
| ------------------- | -------------------- | :---: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---: | :-------: | :-------: | :------: | :------: | :------: |
| Relation DETR       | ResNet50             |  12   | [config](configs/relation_detr/relation_detr_resnet50_800_1333.py) / [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_resnet50_800_1333_coco_1x.pth) / [log](https://github.com/user-attachments/files/16277252/relation_detr_resnet50_800_1333_coco_1x.log) | 51.7  |   69.1    |   56.3    |   36.1   |   55.6   |   66.1   |
| Relation DETR       | Swin-L<sub>(IN-22K)  |  12   |                                                      [config](configs/relation_detr/relation_detr_swin_l_800_1333.py)  /  [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_swin_l_800_1333_coco_1x.pth)                                                      | 57.8  |   76.1    |   62.9    |   41.2   |   62.1   |   74.4   |
| Relation DETR       | ResNet50             |  24   | [config](configs/relation_detr/relation_detr_resnet50_800_1333.py) / [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_resnet50_800_1333_coco_2x.pth) / [log](https://github.com/user-attachments/files/16277261/relation_detr_resnet50_800_1333_coco_2x.log) | 52.1  |   69.7    |   56.6    |   36.1   |   56.0   |   66.5   |
| Relation DETR       | Swin-L<sub>(IN-22K)  |  24   |    [config](configs/relation_detr/relation_detr_swin_l_800_1333.py) / [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_swin_l_800_1333_coco_2x.pth) / [log](https://github.com/user-attachments/files/16277236/relation_detr_swin_l_800_1333_coco_2x.log)    | 58.1  |   76.4    |   63.5    |   41.8   |   63.0   |   73.5   |
| Relation-DETR<sup>‚Ä† | Focal-L<sub>(IN-22K) | 4+24  |                                  [config](configs/relation_detr/relation_detr_focalnet_large_lrf_fl4_1200_2000.py) / [o365_checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_focalnet_large_lrf_fl4_800_1333_o365_4e.pth) / [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_focalnet_large_lrf_fl4_1200_2000_o365_4e-coco_2x.pth)                                  | 63.5  |   80.8    |   69.1    |   47.2   |   66.9   |   77.0   |

‚Ä† means finetuned model on COCO after pretraining on Object365.

**[Other DETR variants:]** We integrate our position relation into existing DETR variants and generate enhanced versions of them. Note some of these weights are newly trained and may produce slightly different results from those reported in our paper. We mark these variants with `++` in the name to distinguish them from their original versions.

| Model             | Backbone | Epoch |                                                                                                Download                                                                                                |  mAP  | AP<sub>50 | AP<sub>75 | AP<sub>S | AP<sub>M | AP<sub>L |
| ----------------- | -------- | :---: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---: | :-------: | :-------: | :------: | :------: | :------: |
| Deformable-DETR++ | ResNet50 |  12   |                                                                  [config](configs/deformable_detr_pp/def_detr_pp_resnet_800_1333.py)                                                                   | 47.0  |   65.6    |   51.2    |   29.3   |   51.0   |   62.2   |
| Dab-Def-DETR++    | ResNet50 |  12   | [config](configs/dab_def_detr_pp/dab_def_detr_pp_resnet50_800_1333.py) / [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.1/dab_def_detr_pp_resnet50_800_1333_coco_1x.pth) | 48.3  |   66.5    |   52.9    |   32.4   |   52.0   |   62.0   |
| DN-Def-DETR++     | ResNet50 |  12   |  [config](configs/dn_def_detr_pp/dn_def_detr_pp_resnet50_800_1333.py) / [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.1/dn_def_detr_pp_resnet50_800_1333_coco_1x.pth)   | 47.3  |   65.6    |   51.4    |   29.9   |   50.8   |   62.1   |
| DINO++            | ResNet50 |  12   |             [config](configs/dino_pp/dino_pp_resnet50_800_1333.py) / [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.1/dino_pp_resnet50_800_1333_coco_1x.pth)             | 50.1  |   67.8    |   54.9    |   33.3   |   53.9   |   63.5   |

### SA-Det-100k

| Model         | Backbone | Epoch |                                                                                                Download                                                                                                 |  mAP  | AP<sub>50 | AP<sub>75 | AP<sub>S | AP<sub>M | AP<sub>L |
| ------------- | -------- | :---: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---: | :-------: | :-------: | :------: | :------: | :------: |
| DINO with VFL | ResNet50 |  12   |                                                                                                   ‚Äî‚Äî                                                                                                    | 43.7  |   52.0    |   47.7    |   5.8    |   43.0   |   61.5   |
| Relation DETR | ResNet50 |  12   | [config](configs/relation_detr/relation_detr_resnet50_800_1333.py) / [checkpoint](https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_resnet50_800_1333_sa_det_100k_1x.pth) | 45.0  |   53.1    |   48.9    |   6.0    |   44.4   |   62.9   |

## Get started

<details>
<summary>1. Installation</summary>

  **We use the environment same as [Salience-DETR](https://arxiv.org/abs/2403.16131). You can skip the step if you have run Salience-DETR.**

  1. Clone the repository:

    ```shell
    git clone https://github.com/xiuqhou/Relation-DETR
    cd Relation-DETR
    ```

  2. Install Pytorch and torchvision:

    ```shell
    conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch
    ```

  3. Install other requirements:

    ```shell
    pip install -r requirements.txt
    ```

</details>

<details>
<summary>2. Prepare datasets</summary>

Download [COCO2017](https://cocodataset.org/) (and [SA-Det-100k](https://huggingface.co/datasets/xiuqhou/SA-Det-100k) optionally), put them in `data/` following the structure:

```shell

data/
  ‚îú‚îÄcoco/
  ‚îÇ  ‚îú‚îÄ‚îÄ train2017/
  ‚îÇ  ‚îú‚îÄ‚îÄ val2017/
  ‚îÇ  ‚îî‚îÄ‚îÄ annotations/
  ‚îÇ         ‚îú‚îÄ‚îÄ instances_train2017.json
  ‚îÇ         ‚îî‚îÄ‚îÄ instances_val2017.json
  ‚îÇ
  ‚îî‚îÄsa_det_100k/
      ‚îú‚îÄ‚îÄ train2017/
      ‚îú‚îÄ‚îÄ val2017/
      ‚îî‚îÄ‚îÄ annotations/
```

</details>

<details>
<summary>3. Evaluate pretrained models</summary>

To evaluate a model with one or more GPUs, specify `CUDA_VISIBLE_DEVICES`, `dataset`, `model` and `checkpoint`.

```shell
CUDA_VISIBLE_DEVICES=<gpu_ids> accelerate launch test.py --coco-path /path/to/coco --model-config /path/to/model.py --checkpoint /path/to/checkpoint.pth
```

For example, run the following shell to evaluate Relation-DETR with ResNet-50 (1x) on COCO, You can expect to get the final AP about 51.7.

```shell
CUDA_VISIBLE_DEVICES=0 accelerate launch test.py \
  --coco-path data/coco \
  --model-config configs/relation_detr/relation_detr_resnet50_800_1333.py \
  --checkpoint https://github.com/xiuqhou/Relation-DETR/releases/download/v1.0.0/relation_detr_resnet50_800_1333_coco_1x.pth
```

- To export results to a json file, specify `--result` with a file name ended with `.json`.
- To visualize predictions, specify `--show-dir` with a folder name. You can change the visualization style through `--font-scale`, `--box-thick`, `--fill-alpha`, `--text-box-color`, `--text-font-color`, `--text-alpha` parameters.
</details>

<details>
<summary>4. Evaluate exported json results</summary>
To evaluate a json results, specify `dataset` and `result`. The evaluation only needs CPU so you don't need to specify `CUDA_VISIBLE_DEVICES`.

```shell
accelerate launch test.py --coco-path /path/to/coco --result /path/to/result.json
```

- To visualize predictions, specify `--show-dir` with a folder name. You can change the visualization style through `--font-scale`, `--box-thick`, `--fill-alpha`, `--text-box-color`, `--text-font-color`, `--text-alpha` parameters.
</details>

<details>
<summary>5. Train a model</summary>

Use `CUDA_VISIBLE_DEVICES` to specify GPU/GPUs and run the following script to start training. If not specified, the script will use all available GPUs on the node to train. Before start training, modify parameters in [configs/train_config.py](configs/train_config.py).

```shell
CUDA_VISIBLE_DEVICES=0 accelerate launch main.py    # train with 1 GPU
CUDA_VISIBLE_DEVICES=0,1 accelerate launch main.py  # train with 2 GPUs
```

</details>

<details>
<summary>5. Benchmark a model</summary>

To test the inference speed, memory cost and parameters of a model, use tools/benchmark_model.py.

```shell
python tools/benchmark_model.py --model-config configs/relation_detr/relation_detr_resnet50_800_1333.py
```

</details>

<details>
<summary>6. Export an ONNX model</summary>

For advanced users who want to deploy our model, we provide a script to export an ONNX file.

```shell
python tools/pytorch2onnx.py \
    --model-config /path/to/model.py \
    --checkpoint /path/to/checkpoint.pth \
    --save-file /path/to/save.onnx \
    --simplify \  # use onnxsim to simplify the exported onnx file
    --verify  # verify the error between onnx model and pytorch model
```

For inference using the ONNX file, see ONNXDetector in [tools/pytorch2onnx.py](tools/pytorch2onnx.py)

</details>

## License

Relation-DETR is released under the Apache 2.0 license. Please see the [LICENSE](LICENSE) file for more information.

## Bibtex

If you find our work helpful for your research, please consider citing:

```bibtex
@inproceedings{hou2024relation,
  title={Relation DETR: Exploring Explicit Position Relation Prior for Object Detection},
  author={Hou, Xiuquan and Liu, Meiqin and Zhang, Senlin and Wei, Ping and Chen, Badong and Lan, Xuguang},
  booktitle={European conference on computer vision},
  year={2024},
  organization={Springer}
}

@InProceedings{Hou_2024_CVPR,
    author    = {Hou, Xiuquan and Liu, Meiqin and Zhang, Senlin and Wei, Ping and Chen, Badong},
    title     = {Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {17574-17583}
}
```
